{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Useful Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym.wrappers as gw\n",
    "#device = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n",
    "from collections import deque \n",
    "import copy\n",
    "from random import randrange\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Random Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent that makes moves at random\n",
    "class RandomAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.data = [] # Index i represents the distance traveled by the agent \n",
    "                       # at iteration i of the last call to run\n",
    "        \n",
    "    # Runs the RandomAgent for iters iterations\n",
    "    def run(self, iters):\n",
    "        print(\"Training initialized for {} episodes\".format(iters))\n",
    "        self.clearData()\n",
    "        for i in range(iters):\n",
    "            done = False\n",
    "            info = None\n",
    "            env.reset()\n",
    "            reward = 0\n",
    "            while not done: # Runs until the environment determines a singular game has ended\n",
    "                action = env.action_space.sample()\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                reward += reward\n",
    "                done = terminated or truncated\n",
    "            print(\"Episode {} reward: {}\".format(i + 1, reward))\n",
    "            self.data.append(reward)\n",
    "    \n",
    "    def getData(self):\n",
    "        return self.data\n",
    "    \n",
    "    def clearData(self):\n",
    "        self.data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the total reward gained by the agent for a performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training initialized for 10 episodes\n",
      "Episode 0 reward: -30\n",
      "Episode 1 reward: -30\n",
      "Episode 2 reward: -30\n",
      "Episode 3 reward: -30\n",
      "Episode 4 reward: -30\n",
      "Episode 5 reward: -30\n",
      "Episode 6 reward: -30\n",
      "Episode 7 reward: -30\n",
      "Episode 8 reward: -30\n",
      "Episode 9 reward: -30\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('SuperMarioBros-v0', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "ra = RandomAgent(env)\n",
    "ra.run(10)\n",
    "env.close()\n",
    "data = ra.getData()\n",
    "\n",
    "plt.plot(data)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Double Deep Q Learning CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SuperMarioBros-v0')#, apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "# Grayscale\n",
    "env = gw.GrayScaleObservation(env, keep_dim=True)\n",
    "# Resizing the environment\n",
    "env = gw.ResizeObservation(env, (84, 84))\n",
    "# Stacking frames for context\n",
    "env = gw.FrameStack(env, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer holds experiences that the network will use for action replay and batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, memory_size=20000):\n",
    "        self.state = deque(maxlen=memory_size)\n",
    "        self.action = deque(maxlen=memory_size)\n",
    "        self.reward = deque(maxlen=memory_size)\n",
    "        self.next_state = deque(maxlen=memory_size)\n",
    "        self.done= deque(maxlen=memory_size)\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.state.append(state)\n",
    "        self.action.append(action)\n",
    "        self.reward.append(reward)\n",
    "        self.next_state.append(next_state)\n",
    "        self.done.append(done)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Deep Q Learning Network using a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNetwork():\n",
    "    def __init__(self, env, alpha = .0001, epsilon = 1, epsilon_decay = .9999, gamma = .99, batch_size = 32,\n",
    "                 update_f = 4, target_update_f = 1024, memory=20000, input_c = 4, n_actions = 7):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.update_f = update_f\n",
    "        self.target_update_f = target_update_f\n",
    "        self.memory = ReplayBuffer(memory)\n",
    "        self.input_c = input_c\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(input_c, 32, kernel_size=6, stride=3) # Produces 27x27x32\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2) # Produces 13x13x64\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1) # Produces 11x11x64\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(11*11*64, 256) # Fully connected layer\n",
    "        self.fc2 = nn.Linear(256, self.n_actions)\n",
    "        \n",
    "        self.model = nn.Sequential(self.conv1, self.bn1, nn.ReLU(), self.conv2, \n",
    "                                   self.bn2, nn.ReLU(), self.conv3, self.bn3, nn.ReLU(), nn.Flatten(), \n",
    "                                   self.fc1, nn.ReLU(), self.fc2)\n",
    "        self.target_model = copy.deepcopy(self.model)\n",
    "        \n",
    "        self.currentEpochLoss = []\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def get_target_model(self):\n",
    "        return self.target_model\n",
    "    \n",
    "    # Adds a transition to the memory for use in the action replay\n",
    "    def add_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append(state, action, reward, next_state, done)\n",
    "    \n",
    "    #Epsilon greedy exploration strategy\n",
    "    def get_action(self, state):\n",
    "        if randrange(0,1) < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.model(state)[0])\n",
    "        \n",
    "    # Returns the best action for a given state\n",
    "    def act(self, state):\n",
    "        return T.argmax(self.model(self.state_to_tensor(state))[0]).item()\n",
    "    \n",
    "    # Converts an env state to a valid tensor\n",
    "    def state_to_tensor(self, state):\n",
    "        return T.FloatTensor(np.array(state)).permute(3, 0, 1, 2)\n",
    "\n",
    "    # Trains the network for n epochs, where each epoch has n max_actions   \n",
    "    def train(self, epochs, max_actions):\n",
    "        for i in range(epochs):\n",
    "            print(\"Epoch {} starting with epsilon {}\".format(i + 1, self.epsilon))\n",
    "                \n",
    "            total_reward = 0\n",
    "            \n",
    "            # Resets the environment\n",
    "            state = env.reset()\n",
    "            \n",
    "            for j in tqdm (range(max_actions), desc=\"Epoch {}\".format(i + 1)):\n",
    "                action = self.get_action(state)\n",
    "                \n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                env.render(mode=\"human\")\n",
    "                total_reward += reward\n",
    "                self.add_memory(state, action, reward, next_state, done)\n",
    "            \n",
    "                if j % self.update_f == 0:\n",
    "                    self.actionReplay()\n",
    "                \n",
    "                if j % self.target_update_f == 0:\n",
    "                    self.target_model = copy.deepcopy(self.model)\n",
    "                    \n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            self.epsilon = max(.01, self.epsilon * self.epsilon_decay)\n",
    "            print(\"Epoch {} finished with a reward of {} and average loss of {}\"\n",
    "                  .format(i + 1, total_reward, sum(self.currentEpochLoss) / len(self.currentEpochLoss)))\n",
    "            self.currentEpochLoss = []\n",
    "                \n",
    "                    \n",
    "    \n",
    "    def actionReplay(self):\n",
    "        indices = np.random.choice(range(len(self.memory)), size=self.batch_size)\n",
    "        \n",
    "        states = np.array([self.memory.state[i] for i in indices])\n",
    "        actions = np.array([self.memory.action[i] for i in indices])\n",
    "        rewards = np.array([self.memory.reward[i] for i in indices])\n",
    "        next_states = np.array([self.memory.next_state[i][0] for i in indices])\n",
    "        done = np.array([self.memory.done[i] for i in indices])\n",
    "        \n",
    "        # Loop completes one batch of action replay\n",
    "        for i in range(0, len(states)):\n",
    "            s = self.state_to_tensor(states[i])\n",
    "            preds = self.model(s)\n",
    "            target_preds = self.target_model(s)\n",
    "        \n",
    "            loss = nn.MSELoss() # Mean Squared Error Loss\n",
    "            optimizer = optim.Adam(self.model.parameters(), self.alpha)\n",
    "        \n",
    "            if done[i]:\n",
    "                preds[0][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                preds[0][actions[i]] = rewards[i] + self.gamma * (T.max(target_preds))\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            output = loss(preds, target_preds)\n",
    "            self.currentEpochLoss.append(output)\n",
    "            output.backward()\n",
    "            optimizer.step()  \n",
    "    \n",
    "    def saveModel(self, path):\n",
    "        T.save(self.model, path)\n",
    "        T.save(self.target_model, \"t_\" + path)\n",
    "        print(\"Successfully save model to {}\".format(path))\n",
    "        \n",
    "    def loadModel(self, path):\n",
    "        self.model = T.load(path)\n",
    "        self.target_model = T.load(\"t_\" + path)\n",
    "        print(\"Successfully loaded model at {}\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent that uses a trained DDQN for determining optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQAgent:\n",
    "    def __init__(self, env, model):\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.data = [] # Index i represents the distance traveled by the agent \n",
    "                       # at iteration i of the last call to run\n",
    "        \n",
    "    # Runs the DDQAgent iters times\n",
    "    def run(self, iters):\n",
    "        self.clearData()\n",
    "        for i in range(iters):\n",
    "            done = False\n",
    "            info = None\n",
    "            obs = env.reset()\n",
    "            t_reward = 0\n",
    "            while not done: # Runs until the environment determines a singular game has ended\n",
    "                action = model.act(obs)\n",
    "                obs, reward, terminated, info = env.step(action)\n",
    "                t_reward += reward\n",
    "                done = terminated\n",
    "                env.render(mode=\"human\")\n",
    "            print(\"Episode {} reward: {}\".format(i + 1, t_reward))\n",
    "            self.data.append(reward)\n",
    "    \n",
    "    def getData(self):\n",
    "        return self.data\n",
    "    \n",
    "    def clearData(self):\n",
    "        self.data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 starting with epsilon 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 500/500 [01:39<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished with a reward of 529.0 and average loss of 0.6393972039222717\n",
      "Epoch 2 starting with epsilon 0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  89%|████████▉ | 446/500 [01:27<00:10,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished with a reward of 712.0 and average loss of 0.5896204113960266\n",
      "Epoch 3 starting with epsilon 0.9998000100000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  60%|██████    | 301/500 [00:55<00:38,  5.19it/s]"
     ]
    }
   ],
   "source": [
    "model = DDQNetwork(env)\n",
    "model.train(50000, 500)\n",
    "model.saveModel(\"smb_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model at smb_1.pt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-d9dec0ecadab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"smb_1.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDQAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-6858dfe37c12>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, iters)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mt_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Runs until the environment determines a singular game has ended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mt_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-6d79b5654f87>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Returns the best action for a given state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Converts an env state to a valid tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[has-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[has-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use cumulative moving average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                     \u001b[0mexponential_average_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = DDQNetwork(env)\n",
    "model.loadModel(\"smb_1.pt\")\n",
    "a = DDQAgent(env, model)\n",
    "a.run(1)\n",
    "env.close()\n",
    "data = a.getData()\n",
    "\n",
    "plt.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
