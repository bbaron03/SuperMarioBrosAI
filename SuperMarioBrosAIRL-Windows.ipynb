{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Useful Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym.wrappers as gw\n",
    "device = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n",
    "from collections import deque \n",
    "import copy\n",
    "from random import randrange\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Random Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent that makes moves at random\n",
    "class RandomAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.data = [] # Index i represents the distance traveled by the agent \n",
    "                       # at iteration i of the last call to run\n",
    "        \n",
    "    # Runs the RandomAgent for iters iterations\n",
    "    def run(self, iters):\n",
    "        print(\"Training initialized for {} episodes\".format(iters))\n",
    "        self.clearData()\n",
    "        for i in range(iters):\n",
    "            done = False\n",
    "            info = None\n",
    "            env.reset()\n",
    "            reward = 0\n",
    "            while not done: # Runs until the environment determines a singular game has ended\n",
    "                action = env.action_space.sample()\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                reward += reward\n",
    "                done = terminated or truncated\n",
    "            print(\"Episode {} reward: {}\".format(i + 1, reward))\n",
    "            self.data.append(reward)\n",
    "    \n",
    "    def getData(self):\n",
    "        return self.data\n",
    "    \n",
    "    def clearData(self):\n",
    "        self.data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the total reward gained by the agent for a performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training initialized for 10 episodes\n",
      "Episode 0 reward: -30\n",
      "Episode 1 reward: -30\n",
      "Episode 2 reward: -30\n",
      "Episode 3 reward: -30\n",
      "Episode 4 reward: -30\n",
      "Episode 5 reward: -30\n",
      "Episode 6 reward: -30\n",
      "Episode 7 reward: -30\n",
      "Episode 8 reward: -30\n",
      "Episode 9 reward: -30\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('SuperMarioBros-v0', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "ra = RandomAgent(env)\n",
    "ra.run(10)\n",
    "env.close()\n",
    "data = ra.getData()\n",
    "\n",
    "plt.plot(data)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Double Deep Q Learning CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bbaro\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\bbaro\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Taken from Yuansong Feng, Suraj Subramanian, Howard Wang, Steven Guo.\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "env = gym.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode=\"human\")\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "# Grayscale\n",
    "env = gw.GrayScaleObservation(env, keep_dim=True)\n",
    "# Resizing the environment\n",
    "env = gw.ResizeObservation(env, (84, 84))\n",
    "# Stacking frames for context\n",
    "env = gw.FrameStack(env, 4)\n",
    "# Skipping n frames \n",
    "env = SkipFrame(env, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer holds experiences that the network will use for action replay and batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, memory_size=20000):\n",
    "        self.state = deque(maxlen=memory_size)\n",
    "        self.action = deque(maxlen=memory_size)\n",
    "        self.reward = deque(maxlen=memory_size)\n",
    "        self.next_state = deque(maxlen=memory_size)\n",
    "        self.done= deque(maxlen=memory_size)\n",
    "    \n",
    "    # Adds a transition\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "        \n",
    "        state = T.tensor(state, device=device, dtype=T.float).permute(3, 0, 1, 2)\n",
    "        next_state = T.tensor(next_state, device=device, dtype=T.float).permute(3, 0, 1, 2)\n",
    "        action = T.tensor([action], device=device, dtype=T.long)\n",
    "        reward = T.tensor([reward], device=device, dtype=T.float)\n",
    "        done = T.tensor([done], device=device, dtype=T.float)\n",
    "        \n",
    "        self.state.append(state)\n",
    "        self.action.append(action)\n",
    "        self.reward.append(reward)\n",
    "        self.next_state.append(next_state)\n",
    "        self.done.append(done)\n",
    "\n",
    "    def length(self):\n",
    "        return len(self.done)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Deep Q Learning Network using a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNetwork():\n",
    "    def __init__(self, env, alpha = .0001, epsilon = 1, epsilon_decay = .99999, gamma = .99, batch_size = 32,\n",
    "                 update_f = 4, target_update_f = 1024, memory=20000, input_c = 4, n_actions = 7):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.update_f = update_f\n",
    "        self.target_update_f = target_update_f\n",
    "        self.save_every = 100000\n",
    "        self.memory = ReplayBuffer(memory)\n",
    "        self.input_c = input_c\n",
    "        self.n_actions = n_actions\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(input_c, 32, kernel_size=6, stride=3) # Produces 27x27x32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2) # Produces 13x13x64\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1) # Produces 11x11x64\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(11*11*64, 512) # Fully connected layer\n",
    "        self.fc2 = nn.Linear(512, self.n_actions)\n",
    "        \n",
    "        self.model = nn.Sequential(self.conv1, nn.ReLU(), self.conv2, \n",
    "                                   nn.ReLU(), self.conv3, nn.ReLU(), nn.Flatten(), \n",
    "                                   self.fc1, nn.ReLU(), self.fc2)\n",
    "        self.target_model = copy.deepcopy(self.model)\n",
    "        \n",
    "        self.model = self.model.to(device=device)\n",
    "        self.target_model = self.target_model.to(device=device)\n",
    "        for p in self.target_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.currentEpochLoss = []\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), self.alpha)\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def get_target_model(self):\n",
    "        return self.target_model\n",
    "    \n",
    "    # Adds a transition to the memory for use in the action replay\n",
    "    def add_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append(state, action, reward, next_state, done)\n",
    "    \n",
    "    #Epsilon greedy exploration strategy\n",
    "    def get_action(self, state):\n",
    "        if randrange(0,1) < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            a = np.argmax(self.model(state)[0])\n",
    "            return a\n",
    "        \n",
    "    # Returns the best action for a given state\n",
    "    def act(self, state):\n",
    "        a = T.argmax(self.model(self.state_to_tensor(state))[0]).item()\n",
    "        return a\n",
    "    \n",
    "    # Converts an env state to a valid tensor\n",
    "    def state_to_tensor(self, state):\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        t = T.tensor(np.array(state, dtype=np.float), dtype=T.float).permute(3, 0, 1, 2)\n",
    "        t = t.to(device=device)\n",
    "        return t\n",
    "\n",
    "    # Trains the network for n epochs\n",
    "    def train(self, epochs):\n",
    "        for i in range(epochs):\n",
    "            print(\"Epoch {} starting with epsilon {}\".format(i + 1, self.epsilon))\n",
    "                \n",
    "            total_reward = 0\n",
    "            j = 0\n",
    "            # Resets the environment\n",
    "            state = env.reset()\n",
    "            \n",
    "            while True:\n",
    "                action = self.get_action(state)\n",
    "                \n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "                \n",
    "                self.add_memory(state, action, reward, next_state, done)\n",
    "            \n",
    "                if self.total_steps % self.update_f == 0 and self.total_steps != 0 and self.memory.length() > self.batch_size:\n",
    "                    self.actionReplay()\n",
    "                \n",
    "                if self.total_steps % self.target_update_f == 0 and self.total_steps !=0:\n",
    "                    self.target_model.load_state_dict(self.model.state_dict())\n",
    "                    \n",
    "                if self.total_steps % self.save_every == 0 and self.total_steps !=0:\n",
    "                    self.saveModel(\"model.pt\")\n",
    "                    \n",
    "                state = next_state\n",
    "                j += 1\n",
    "                self.total_steps += 1\n",
    "                if done:\n",
    "                    break\n",
    "               \n",
    "            self.epsilon = max(.01, self.epsilon * self.epsilon_decay)\n",
    "            print(\"Epoch {} finished in {} steps with a reward of {} and average loss of {}\"\n",
    "                  .format(i + 1, j, total_reward, sum(self.currentEpochLoss) / len(self.currentEpochLoss)))\n",
    "            self.currentEpochLoss = []\n",
    "                \n",
    "                    \n",
    "    # Performs one iteration of action replay of the given batch size. Replays states from agents memory\n",
    "    # and compares its predictions with the expected results gathered from the target model.\n",
    "    def actionReplay(self):\n",
    "        indices = np.random.choice(range(len(self.memory)), size=self.batch_size)\n",
    "        \n",
    "        states = [self.memory.state[i] for i in indices]\n",
    "        actions = [self.memory.action[i] for i in indices]\n",
    "        rewards = [self.memory.reward[i] for i in indices]\n",
    "        next_states = [self.memory.next_state[i] for i in indices]\n",
    "        done = [self.memory.done[i] for i in indices]\n",
    "        \n",
    "        # Loop completes one batch of action replay\n",
    "        for i in range(0, len(states)):\n",
    "            s = states[i]\n",
    "            ns = next_states[i]\n",
    "            a = actions[i]\n",
    "            current_q = self.model(s)\n",
    "            next_a = T.argmax(self.model(ns))\n",
    "            next_q = self.target_model(ns).detach()\n",
    "        \n",
    "            expected = T.clone(current_q).detach()\n",
    "            \n",
    "            if done[i]:\n",
    "                expected[0][next_a] = rewards[i]\n",
    "            else:\n",
    "                expected[0][next_a] = rewards[i] + self.gamma * T.max(next_q)\n",
    "            \n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss_fn(current_q, expected)\n",
    "            self.currentEpochLoss.append(loss)\n",
    "            loss.backward()\n",
    "            self.optimizer.step() \n",
    "            \n",
    "    # Saves the models to separate .pt files. Target network is labeled with prefix \"t_\"\n",
    "    def saveModel(self, path):\n",
    "        T.save(self.model, \"checkpoint_{}_{}\".format(self.total_steps, path))\n",
    "        T.save(self.target_model, \"t_checkpoint_{}_{}\".format(self.total_steps, path))\n",
    "        print(\"Successfully save model to {}\".format(path))\n",
    "    \n",
    "    # Loads the models from .pt files of given path\n",
    "    def loadModel(self, path):\n",
    "        self.model = T.load(path)\n",
    "        self.target_model = T.load(\"t_\" + path)\n",
    "        self.model = self.model.to(device=device)\n",
    "        self.target_model = self.target_model.to(device=device)\n",
    "        print(\"Successfully loaded model at {}\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent that uses a trained DDQN for determining optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQAgent:\n",
    "    def __init__(self, env, model):\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.data = [] # Index i represents the distance traveled by the agent \n",
    "                       # at iteration i of the last call to run\n",
    "        \n",
    "    # Runs the DDQAgent iters times\n",
    "    def run(self, iters):\n",
    "        self.clearData()\n",
    "        for i in range(iters):\n",
    "            done = False\n",
    "            info = None\n",
    "            obs = env.reset()\n",
    "            t_reward = 0\n",
    "            while not done: # Runs until the environment determines a singular game has ended\n",
    "                action = model.act(obs)\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                t_reward += reward\n",
    "                done = terminated or truncated\n",
    "            print(\"Episode {} reward: {}\".format(i + 1, t_reward))\n",
    "            self.data.append(reward)\n",
    "    \n",
    "    # Gets the reward data list\n",
    "    def getData(self):\n",
    "        return self.data\n",
    "    \n",
    "    # Resets the reward data list\n",
    "    def clearData(self):\n",
    "        self.data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model at checkpoint_5000000_model.pt\n",
      "Epoch 1 starting with epsilon 1\n",
      "Epoch 1 finished in 996 steps with a reward of 1460.0 and average loss of 0.3796137273311615\n",
      "Epoch 2 starting with epsilon 0.99999\n",
      "Epoch 2 finished in 433 steps with a reward of 1449.0 and average loss of 0.5490546822547913\n",
      "Epoch 3 starting with epsilon 0.9999800001000001\n",
      "Epoch 3 finished in 1047 steps with a reward of 2147.0 and average loss of 0.6991651654243469\n",
      "Epoch 4 starting with epsilon 0.9999700002999992\n",
      "Epoch 4 finished in 813 steps with a reward of 1795.0 and average loss of 0.7458937168121338\n",
      "Epoch 5 starting with epsilon 0.9999600005999962\n",
      "Epoch 5 finished in 1781 steps with a reward of 2167.0 and average loss of 0.6927570700645447\n",
      "Epoch 6 starting with epsilon 0.9999500009999903\n",
      "Epoch 6 finished in 3398 steps with a reward of 1657.0 and average loss of 0.5314425826072693\n",
      "Epoch 7 starting with epsilon 0.9999400014999804\n",
      "Epoch 7 finished in 822 steps with a reward of 1806.0 and average loss of 0.4772825539112091\n",
      "Epoch 8 starting with epsilon 0.9999300020999654\n",
      "Epoch 8 finished in 1095 steps with a reward of 1364.0 and average loss of 0.4943806827068329\n",
      "Epoch 9 starting with epsilon 0.9999200027999444\n",
      "Epoch 9 finished in 2416 steps with a reward of 2245.0 and average loss of 0.44363969564437866\n",
      "Epoch 10 starting with epsilon 0.9999100035999164\n",
      "Epoch 10 finished in 704 steps with a reward of 1534.0 and average loss of 0.48129647970199585\n",
      "Epoch 11 starting with epsilon 0.9999000044998805\n",
      "Epoch 11 finished in 1239 steps with a reward of 2694.0 and average loss of 0.4835425615310669\n",
      "Epoch 12 starting with epsilon 0.9998900054998355\n",
      "Epoch 12 finished in 949 steps with a reward of 1737.0 and average loss of 0.47513213753700256\n",
      "Epoch 13 starting with epsilon 0.9998800065997806\n",
      "Epoch 13 finished in 1599 steps with a reward of 2157.0 and average loss of 0.4668794274330139\n",
      "Epoch 14 starting with epsilon 0.9998700077997147\n",
      "Epoch 14 finished in 235 steps with a reward of 1035.0 and average loss of 0.5471200942993164\n",
      "Epoch 15 starting with epsilon 0.9998600090996367\n",
      "Epoch 15 finished in 1487 steps with a reward of 2439.0 and average loss of 0.5149693489074707\n",
      "Epoch 16 starting with epsilon 0.9998500104995457\n",
      "Epoch 16 finished in 2103 steps with a reward of 851.0 and average loss of 0.455771803855896\n",
      "Epoch 17 starting with epsilon 0.9998400119994407\n",
      "Epoch 17 finished in 786 steps with a reward of 1873.0 and average loss of 0.46777409315109253\n",
      "Epoch 18 starting with epsilon 0.9998300135993208\n",
      "Epoch 18 finished in 664 steps with a reward of 1500.0 and average loss of 0.4517146944999695\n",
      "Epoch 19 starting with epsilon 0.9998200152991848\n",
      "Epoch 19 finished in 1298 steps with a reward of 1961.0 and average loss of 0.46175047755241394\n",
      "Epoch 20 starting with epsilon 0.999810017099032\n",
      "Epoch 20 finished in 1730 steps with a reward of 2398.0 and average loss of 0.41433078050613403\n",
      "Epoch 21 starting with epsilon 0.999800018998861\n",
      "Epoch 21 finished in 844 steps with a reward of 1079.0 and average loss of 0.414354145526886\n",
      "Epoch 22 starting with epsilon 0.999790020998671\n",
      "Epoch 22 finished in 2897 steps with a reward of 1964.0 and average loss of 0.47181078791618347\n",
      "Epoch 23 starting with epsilon 0.999780023098461\n",
      "Epoch 23 finished in 294 steps with a reward of 1078.0 and average loss of 0.4407607913017273\n",
      "Epoch 24 starting with epsilon 0.9997700252982301\n",
      "Epoch 24 finished in 1086 steps with a reward of 2074.0 and average loss of 0.5144728422164917\n",
      "Epoch 25 starting with epsilon 0.9997600275979772\n",
      "Epoch 25 finished in 734 steps with a reward of 1085.0 and average loss of 0.4749922752380371\n",
      "Epoch 26 starting with epsilon 0.9997500299977012\n",
      "Epoch 26 finished in 285 steps with a reward of 1077.0 and average loss of 0.4811423420906067\n",
      "Epoch 27 starting with epsilon 0.9997400324974013\n",
      "Epoch 27 finished in 1547 steps with a reward of 2171.0 and average loss of 0.4977063536643982\n",
      "Epoch 28 starting with epsilon 0.9997300350970764\n",
      "Epoch 28 finished in 1079 steps with a reward of 2274.0 and average loss of 0.46172288060188293\n",
      "Epoch 29 starting with epsilon 0.9997200377967255\n",
      "Epoch 29 finished in 684 steps with a reward of 1617.0 and average loss of 0.45525825023651123\n",
      "Epoch 30 starting with epsilon 0.9997100405963475\n",
      "Epoch 30 finished in 168 steps with a reward of 625.0 and average loss of 0.48818567395210266\n",
      "Epoch 31 starting with epsilon 0.9997000434959415\n",
      "Epoch 31 finished in 3024 steps with a reward of 2059.0 and average loss of 0.45377084612846375\n",
      "Epoch 32 starting with epsilon 0.9996900464955066\n",
      "Epoch 32 finished in 531 steps with a reward of 1878.0 and average loss of 0.5069299340248108\n",
      "Epoch 33 starting with epsilon 0.9996800495950418\n",
      "Epoch 33 finished in 2304 steps with a reward of 1712.0 and average loss of 0.45165789127349854\n",
      "Epoch 34 starting with epsilon 0.9996700527945458\n",
      "Epoch 34 finished in 264 steps with a reward of 1094.0 and average loss of 0.45939573645591736\n",
      "Epoch 35 starting with epsilon 0.9996600560940179\n",
      "Epoch 35 finished in 1101 steps with a reward of 2177.0 and average loss of 0.4926641881465912\n",
      "Epoch 36 starting with epsilon 0.999650059493457\n",
      "Epoch 36 finished in 1053 steps with a reward of 1021.0 and average loss of 0.47538164258003235\n",
      "Epoch 37 starting with epsilon 0.9996400629928621\n",
      "Epoch 37 finished in 1472 steps with a reward of 1552.0 and average loss of 0.48419302701950073\n",
      "Epoch 38 starting with epsilon 0.9996300665922322\n",
      "Epoch 38 finished in 257 steps with a reward of 997.0 and average loss of 0.44202786684036255\n",
      "Epoch 39 starting with epsilon 0.9996200702915663\n",
      "Epoch 39 finished in 627 steps with a reward of 1801.0 and average loss of 0.47323963046073914\n",
      "Epoch 40 starting with epsilon 0.9996100740908634\n",
      "Epoch 40 finished in 938 steps with a reward of 1856.0 and average loss of 0.5196701288223267\n",
      "Epoch 41 starting with epsilon 0.9996000779901226\n",
      "Epoch 41 finished in 2000 steps with a reward of 1098.0 and average loss of 0.4735586941242218\n",
      "Epoch 42 starting with epsilon 0.9995900819893427\n",
      "Epoch 42 finished in 324 steps with a reward of 1225.0 and average loss of 0.5117045044898987\n",
      "Epoch 43 starting with epsilon 0.9995800860885229\n",
      "Epoch 43 finished in 195 steps with a reward of 1071.0 and average loss of 0.4634849429130554\n",
      "Epoch 44 starting with epsilon 0.9995700902876621\n",
      "Epoch 44 finished in 522 steps with a reward of 1402.0 and average loss of 0.4709632098674774\n",
      "Epoch 45 starting with epsilon 0.9995600945867592\n",
      "Epoch 45 finished in 785 steps with a reward of 1516.0 and average loss of 0.44280219078063965\n",
      "Epoch 46 starting with epsilon 0.9995500989858134\n",
      "Epoch 46 finished in 446 steps with a reward of 1551.0 and average loss of 0.47804275155067444\n",
      "Epoch 47 starting with epsilon 0.9995401034848236\n",
      "Epoch 47 finished in 1162 steps with a reward of 1449.0 and average loss of 0.4612804353237152\n",
      "Epoch 48 starting with epsilon 0.9995301080837887\n",
      "Epoch 48 finished in 1449 steps with a reward of 1379.0 and average loss of 0.44802427291870117\n",
      "Epoch 49 starting with epsilon 0.9995201127827079\n",
      "Epoch 49 finished in 1487 steps with a reward of 2053.0 and average loss of 0.4541492760181427\n",
      "Epoch 50 starting with epsilon 0.9995101175815801\n",
      "Epoch 50 finished in 1974 steps with a reward of 2115.0 and average loss of 0.5120154023170471\n",
      "Epoch 51 starting with epsilon 0.9995001224804044\n",
      "Epoch 51 finished in 2009 steps with a reward of 1142.0 and average loss of 0.49854710698127747\n",
      "Epoch 52 starting with epsilon 0.9994901274791796\n",
      "Epoch 52 finished in 498 steps with a reward of 1106.0 and average loss of 0.4349978268146515\n",
      "Epoch 53 starting with epsilon 0.9994801325779049\n",
      "Epoch 53 finished in 279 steps with a reward of 1037.0 and average loss of 0.4413788914680481\n",
      "Epoch 54 starting with epsilon 0.9994701377765792\n",
      "Epoch 54 finished in 1902 steps with a reward of 2598.0 and average loss of 0.47152772545814514\n",
      "Epoch 55 starting with epsilon 0.9994601430752015\n",
      "Epoch 55 finished in 977 steps with a reward of 1472.0 and average loss of 0.43530189990997314\n",
      "Epoch 56 starting with epsilon 0.9994501484737708\n",
      "Epoch 56 finished in 2140 steps with a reward of 2253.0 and average loss of 0.4556038975715637\n",
      "Epoch 57 starting with epsilon 0.9994401539722861\n",
      "Epoch 57 finished in 2164 steps with a reward of 1437.0 and average loss of 0.44116660952568054\n",
      "Epoch 58 starting with epsilon 0.9994301595707464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 finished in 1338 steps with a reward of 1841.0 and average loss of 0.41339603066444397\n",
      "Epoch 59 starting with epsilon 0.9994201652691508\n",
      "Epoch 59 finished in 671 steps with a reward of 1578.0 and average loss of 0.48921486735343933\n",
      "Epoch 60 starting with epsilon 0.9994101710674982\n",
      "Epoch 60 finished in 733 steps with a reward of 1050.0 and average loss of 0.4517686665058136\n",
      "Epoch 61 starting with epsilon 0.9994001769657875\n",
      "Epoch 61 finished in 956 steps with a reward of 1613.0 and average loss of 0.44943177700042725\n",
      "Epoch 62 starting with epsilon 0.9993901829640179\n",
      "Epoch 62 finished in 131 steps with a reward of 702.0 and average loss of 0.4621625542640686\n",
      "Epoch 63 starting with epsilon 0.9993801890621884\n",
      "Epoch 63 finished in 494 steps with a reward of 1602.0 and average loss of 0.3667326271533966\n",
      "Epoch 64 starting with epsilon 0.9993701952602978\n",
      "Epoch 64 finished in 1235 steps with a reward of 3126.0 and average loss of 0.48423337936401367\n",
      "Epoch 65 starting with epsilon 0.9993602015583453\n",
      "Epoch 65 finished in 695 steps with a reward of 1540.0 and average loss of 0.39087510108947754\n",
      "Epoch 66 starting with epsilon 0.9993502079563297\n",
      "Epoch 66 finished in 1366 steps with a reward of 1256.0 and average loss of 0.3813715875148773\n",
      "Epoch 67 starting with epsilon 0.9993402144542501\n",
      "Epoch 67 finished in 1383 steps with a reward of 1843.0 and average loss of 0.4473596513271332\n",
      "Epoch 68 starting with epsilon 0.9993302210521057\n"
     ]
    }
   ],
   "source": [
    "model = DDQNetwork(env)\n",
    "model.loadModel(\"checkpoint_5000000_model.pt\")\n",
    "model.train(5000)\n",
    "model.saveModel(\"smb_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model at checkpoint_5000000_model.pt\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bbaro\\AppData\\Local\\Temp/ipykernel_18276/324960426.py:66: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  t = T.tensor(np.array(state, dtype=np.float), dtype=T.float).permute(3, 0, 1, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "Episode 1 reward: 680.0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "Episode 2 reward: 680.0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "Episode 3 reward: 680.0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "Episode 4 reward: 680.0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "Episode 5 reward: 680.0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "Episode 6 reward: 680.0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "Episode 7 reward: 680.0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "Episode 8 reward: 680.0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "Episode 9 reward: 680.0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "Episode 10 reward: 680.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = DDQNetwork(env)\n",
    "model.loadModel(\"checkpoint_5000000_model.pt\")\n",
    "a = DDQAgent(env, model)\n",
    "a.run(10)\n",
    "data = a.getData()\n",
    "\n",
    "plt.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "T.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
